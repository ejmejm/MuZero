from __future__ import annotations

import collections
import numpy as np
from typing import Optional

MAXIMUM_FLOAT_VALUE = float('inf')
KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])

class MinMaxStats(object):
  """A class that holds the min-max values of the tree."""

  def __init__(self, known_bounds: Optional[KnownBounds]):
    self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE
    self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE

  def update(self, value: float):
    self.maximum = max(self.maximum, value)
    self.minimum = min(self.minimum, value)

  def normalize(self, value: float) -> float:
    if self.maximum > self.minimum:
      # We normalize only when we have set the maximum and minimum values.
      return (value - self.minimum) / (self.maximum - self.minimum)
    return value

class Node(object):

  def __init__(self, prior: float, parent: Optional[Node] = None):
    self.visit_count = 0
    self.to_play = -1
    self.prior = prior
    self.value_sum = 0
    self.children = {} # Maps Action to Node(s)?
    self.parent = parent
    self.hidden_state = None
    self.reward = 0 # Reward returned moving into this step (not from)

  def expanded(self) -> bool:
    return len(self.children) > 0

  def value(self) -> float:
    if self.visit_count == 0:
      return 0
    return self.value_sum / self.visit_count

  # Select which action to take after MCTS simulations are over
  def select_action(self, config: MuZeroConfig, num_moves: int, network: Network):
    visit_counts = [
        (child.visit_count, action) for action, child in self.children.items()
    ]
    t = config.visit_softmax_temperature_fn(
        num_moves=num_moves, training_steps=network.training_steps())
    _, action = softmax_sample(visit_counts, t)
    return action

  # Select the child with the highest UCB score.
  def select_child(self, config: MuZeroConfig, min_max_stats: MinMaxStats):
    _, action, child = max(
        (child.ucb_score(config, min_max_stats), action, child) \
        for action, child in self.children.items())
    return action, child

  # The score for a node is based on its value, plus an exploration bonus based on
  # the prior.
  def ucb_score(self, config: MuZeroConfig, min_max_stats: MinMaxStats) -> float:
    # Wolfram Alpha graph input: graph sqrt(x + 10) / (x + 1) * (ln((x + 10 + 19652 + 1) / 19652) + 1.25) on [0, 10]
    pb_c = np.log((self.parent.visit_count + config.pb_c_base + 1) /
                    config.pb_c_base) + config.pb_c_init
    pb_c *= np.sqrt(self.parent.visit_count) / (self.visit_count + 1)

    prior_score = pb_c * self.prior
    if self.visit_count > 0:
      # TODO: This looks wrong because value is sclaed but reward is not, look into this
      # Do note that the reward should always be generated by a NN, which may be scaled
      # Graph network output scaling: graph (sign(x) * (sqrt(abs(x) + 1) - 1 + 0.001 * x)) on [-100, 100]

      # A new note, this is actually a Q-value of the parent state with the action for this child node 
      q_score = self.reward + config.discount * min_max_stats.normalize(self.value())
    else:
      # Why default to 0 if the node has not been visited yet?
      # Likely because it is prohibitively expensive to generate values for all possible children in the current format.
      # That would be essentially expnding all children nodes.
      # In this case the decision will be only based off high prior and low visit count.
      # TODO: Try this with a network that predicts all Q-values instead of just value for a node.
      # The Q-value can then be derived with an argmax over Q-values, maybe using double dueling architecture.
      q_score = 0
    return q_score + prior_score


  # We expand a node using the value, reward and policy prediction obtained from
  # the neural network.
  def expand(self, to_play: Player, actions: List[Action],
             network_output: NetworkOutput):
    self.to_play = to_play
    self.hidden_state = network_output.hidden_state
    self.reward = network_output.reward
    # Softmax for policy is calculated in expand make sure not to recalculate it
    policy = {a: np.exp(network_output.policy_logits[a.index]) for a in actions}
    policy_sum = sum(policy.values())
    for action, p in policy.items():
      self.children[action] = Node(p / policy_sum, self)


  # At the end of a simulation, we propagate the evaluation all the way up the
  # tree to the root.
  def backpropagate(self, value: float, to_play: Player,
                    discount: float, min_max_stats: MinMaxStats):
    self.value_sum += value if self.to_play == to_play else -value
    self.visit_count += 1
    min_max_stats.update(self.value())

    # Technically this is the Q-value, but because you end up averaging
    # over many Q-values, it will end up being an estimate of the value
    value = self.reward + discount * value

    if self.parent is not None:
      self.parent.backpropagate(value, to_play, discount, min_max_stats)


  # At the start of each search, we add dirichlet noise to the prior of the root
  # to encourage the search to explore new actions.
  def add_exploration_noise(self, config: MuZeroConfig):
    actions = list(self.children.keys())
    noise = np.random.dirichlet([config.root_dirichlet_alpha] * len(actions))
    frac = config.root_exploration_fraction
    for a, n in zip(actions, noise):
      self.children[a].prior = self.children[a].prior * (1 - frac) + n * frac


def softmax_sample(distribution, temperature: float):
  scaled_visit_counts = [visit_count / temperature for visit_count, _ in distribution]
  actions = [action for _, action in distribution]

  # TODO: recode this to make sure there is no overflow
  denominator = np.sum(np.exp(scaled_visit_counts))
  if denominator == np.inf:
    raise ValueError('Softmax resulted in inifinity!')

  probabilities = np.exp(scaled_visit_counts) / denominator
  action_idx = np.random.choice(range(len(actions)), p=probabilities)

  return probabilities[action_idx], actions[action_idx]

# Core Monte Carlo Tree Search algorithm.
# To decide on an action, we run N simulations, always starting at the root of
# the search tree and traversing the tree according to the UCB formula until we
# reach a leaf node.
def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,
             network: Network):
  min_max_stats = MinMaxStats(config.known_bounds)

  for _ in range(config.num_simulations):
    history = action_history.clone()
    node = root
    
    while node.expanded():
      action, node = node.select_child(config, min_max_stats)
      history.add_action(action)

    # Inside the search tree we use the dynamics function to obtain the next
    # hidden state given an action and the previous hidden state.
    parent = node.parent
    network_output = network.recurrent_inference(parent.hidden_state,
                                                 history.last_action())
    network_output = network_output.numpy()
    node.expand(history.to_play(), history.action_space(), network_output)
    node.backpropagate(network_output.value, history.to_play(),
                       config.discount, min_max_stats)